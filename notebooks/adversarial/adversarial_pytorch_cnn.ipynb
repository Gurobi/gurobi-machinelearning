{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f1cbcfa",
   "metadata": {},
   "source": [
    "# Adversarial Example with PyTorch CNN\n",
    "\n",
    "This notebook demonstrates how to embed a trained PyTorch CNN into a Gurobi model using `gurobi_ml` and construct an adversarial example.\n",
    "\n",
    "We train a small CNN on MNIST, then, given a correctly classified image, we formulate a MILP that seeks a nearby image (in L1 norm) that flips the predicted label by maximizing the margin between a target wrong class and the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80408f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import gurobipy as gp\n",
    "from gurobi_ml import add_predictor_constr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afe5a8",
   "metadata": {},
   "source": [
    "## Define CNN model\n",
    "\n",
    "We use only layers supported by the MILP embedding: Conv2d (padding=0), ReLU, MaxPool2d, Flatten, and Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d -> ReLU -> MaxPool2d -> Conv2d -> ReLU -> MaxPool2d -> Flatten -> Linear -> ReLU -> Linear\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 10, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(10, 20, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(20 * 5 * 5, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10),\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facae133",
   "metadata": {},
   "source": [
    "## Load MNIST data\n",
    "\n",
    "Note: The first run may download MNIST. If running in an offline environment, ensure MNIST is cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()  # scales to [0,1]\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d83367",
   "metadata": {},
   "source": [
    "## Quick training (optional)\n",
    "\n",
    "A single epoch is enough for demonstration. You can increase epochs for better accuracy, or skip and load pre-trained weights if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = int(os.environ.get(\"EPOCHS\", \"1\"))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "    print(f\"Epoch {epoch+1}: loss={running/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "# Evaluate quickly\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        logits = model(xb.to(device))\n",
    "        pred = logits.argmax(dim=1).cpu()\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "print(f\"Test accuracy: {correct/total:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e0148",
   "metadata": {},
   "source": [
    "## Select a correctly classified example\n",
    "\n",
    "We pick the first test example that the model currently classifies correctly, and define a target wrong label as the second-best logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faadde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = None\n",
    "true_label = None\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        logits = model(xb.to(device))\n",
    "        pred = logits.argmax(dim=1).cpu()\n",
    "        mask = pred == yb\n",
    "        if mask.any():\n",
    "            idx = mask.nonzero(as_tuple=False)[0].item()\n",
    "            example_img = xb[idx : idx + 1].cpu()  # shape (1,1,28,28)\n",
    "            true_label = yb[idx].item()\n",
    "            break\n",
    "\n",
    "assert example_img is not None, \"No correctly classified example found.\"\n",
    "logits = model(example_img).squeeze(0)\n",
    "probs = torch.softmax(logits, dim=0).cpu().detach().numpy()\n",
    "sorted_labels = np.argsort(probs)\n",
    "right_label = int(sorted_labels[-1])\n",
    "wrong_label = (\n",
    "    int(sorted_labels[-2])\n",
    "    if int(sorted_labels[-1]) == true_label\n",
    "    else int(sorted_labels[-1])\n",
    ")\n",
    "print(f\"True label={true_label}, predicted={right_label}, target wrong={wrong_label}\")\n",
    "plt.imshow(example_img.squeeze(0).squeeze(0), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b995a1",
   "metadata": {},
   "source": [
    "## Build MILP for adversarial example\n",
    "\n",
    "We use an L1-ball constraint around the original image and maximize the margin `y[target] - y[true]`.\n",
    "\n",
    "Important: The MILP embedding expects NHWC input (batch,height,width,channels). We convert the MNIST sample accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ca52a-4168-4202-9e9e-6ff77f2a9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare example and shapes\n",
    "example_np_nhwc = (\n",
    "    example_img.squeeze(0).permute(1, 2, 0).cpu().numpy()[None, ...]\n",
    ")  # (1,28,28,1)\n",
    "\n",
    "m = gp.Model()\n",
    "delta = 0.001  # L1 radius (tune as desired)\n",
    "\n",
    "x = m.addMVar(example_np_nhwc.shape, lb=0.0, ub=1.0, name=\"x\")\n",
    "y = m.addMVar((1, 10), lb=-gp.GRB.INFINITY, name=\"y\")\n",
    "\n",
    "abs_diff = m.addMVar(example_np_nhwc.shape, lb=0.0, ub=1.0, name=\"abs_diff\")\n",
    "m.setObjective(y[0, wrong_label] - y[0, right_label], gp.GRB.MAXIMIZE)\n",
    "\n",
    "# L1-ball constraints\n",
    "m.addConstr(abs_diff >= x - example_np_nhwc)\n",
    "m.addConstr(abs_diff >= -x + example_np_nhwc)\n",
    "m.addConstr(abs_diff.sum() <= delta)\n",
    "\n",
    "# Embed the PyTorch CNN (nn.Sequential)\n",
    "pred_constr = add_predictor_constr(m, model, x, y)\n",
    "pred_constr.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6f31c-5a58-4873-a1d2-a67745fd22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_constr.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f968a-fb32-4735-a4a6-e2053d593d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.Start = example_np_nhwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5da81d-4be4-4dcc-b701-d8ffb91583c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_constr.layers[2].input.lb = 0.0\n",
    "pred_constr.layers[2].input.ub = 10.0\n",
    "pred_constr.layers[5].input.lb = 0.0\n",
    "pred_constr.layers[5].input.ub = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3ebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping: stop when a counterexample is found or proven impossible\n",
    "m.Params.BestBdStop = 0.0\n",
    "m.Params.BestObjStop = 0.0\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5fd6a",
   "metadata": {},
   "source": [
    "## Visualize result\n",
    "\n",
    "Plot the adversarial image and the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if m.SolCount > 0:\n",
    "    x_adv = x.X.squeeze(0)  # (28,28,1)\n",
    "    x_adv_img = x_adv[..., 0]\n",
    "    pert = x_adv_img - example_np_nhwc.squeeze(0)[..., 0]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "    axs[0].set_title(\"Original\")\n",
    "    axs[0].imshow(example_np_nhwc.squeeze(0)[..., 0], cmap=\"gray\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].set_title(\"Adversarial\")\n",
    "    axs[1].imshow(x_adv_img, cmap=\"gray\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[2].set_title(\"Perturbation\")\n",
    "    im = axs[2].imshow(pert, cmap=\"bwr\")\n",
    "    axs[2].axis(\"off\")\n",
    "    plt.colorbar(im, ax=axs[2], fraction=0.046, pad=0.04)\n",
    "    plt.show()\n",
    "\n",
    "    # Check model's prediction on adversarial image\n",
    "    with torch.no_grad():\n",
    "        t_in = (\n",
    "            torch.from_numpy(x_adv[None, ...]).float().permute(0, 3, 1, 2)\n",
    "        )  # NHWC -> NCHW\n",
    "        logits_adv = model(t_in)\n",
    "        pred_adv = logits_adv.argmax(dim=1).item()\n",
    "    print(f\"Adversarial predicted label: {pred_adv}\")\n",
    "else:\n",
    "    print(\"No adversarial example found within given L1 radius.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb3162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix input to the original example and check embedding error\n",
    "# This cell ensures get_error() is ~0 after the PyTorch CNN fix.\n",
    "try:\n",
    "    x.lb = example_np_nhwc\n",
    "    x.ub = example_np_nhwc\n",
    "except Exception as e:\n",
    "    print(\"Failed to set bounds on x:\", e)\n",
    "\n",
    "# Optimize with tiny time limit; only feasibility is needed\n",
    "m.setObjective(0.0)\n",
    "m.setParam(\"TimeLimit\", 1.0)\n",
    "m.optimize()\n",
    "\n",
    "\n",
    "err = pred_constr.get_error()\n",
    "print(\"Max error:\", float(np.max(err.astype(float))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0397c-a5b5-4249-a7f8-273823ad1707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "license": {
   "full_text": "# Copyright Â© 2023 Gurobi Optimization, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================="
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a00ac5",
   "metadata": {},
   "source": [
    "# Price Optimization\n",
    "\n",
    "This example is adapted from the example in Gurobi's modeling examples [How Much\n",
    "Is Too Much? Avocado Pricing and Supply Using Mathematical\n",
    "Optimization](https://github.com/Gurobi/modeling-examples/tree/master/price_optimization).\n",
    "\n",
    "We develop the same example as in the documentation but we try and compare different\n",
    "regression models to estimate demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e1ee1-637b-47aa-93be-c140df0610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "\n",
    "from gurobi_ml import add_predictor_constr\n",
    "import gurobipy_pandas as gppd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ed7e0",
   "metadata": {},
   "source": [
    "## Load the Packages and the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666167a-ce1d-4647-be7e-1f4a56f8dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/price_optimization/\"\n",
    "avocado = pd.read_csv(\n",
    "    data_url + \"HABdata_2019_2022.csv\"\n",
    ")  # dataset downloaded directly from HAB\n",
    "avocado_old = pd.read_csv(\n",
    "    data_url + \"kaggledata_till2018.csv\"\n",
    ")  # dataset downloaded from Kaggle\n",
    "avocado = pd.concat([avocado, avocado_old])\n",
    "\n",
    "# Add the index for each year from 2015 through 2022\n",
    "avocado[\"date\"] = pd.to_datetime(avocado[\"date\"])\n",
    "avocado[\"year\"] = pd.DatetimeIndex(avocado[\"date\"]).year\n",
    "avocado[\"year_index\"] = avocado[\"year\"] - 2015\n",
    "avocado = avocado.sort_values(by=\"date\")\n",
    "\n",
    "# Define the peak season\n",
    "avocado[\"month\"] = pd.DatetimeIndex(avocado[\"date\"]).month\n",
    "peak_months = range(2, 8)  # <--------- Set the months for the \"peak season\"\n",
    "\n",
    "\n",
    "def peak_season(row):\n",
    "    return 1 if int(row[\"month\"]) in peak_months else 0\n",
    "\n",
    "\n",
    "avocado[\"peak\"] = avocado.apply(lambda row: peak_season(row), axis=1)\n",
    "\n",
    "# Scale the number of avocados to millions\n",
    "avocado[\"units_sold\"] = avocado[\"units_sold\"] / 1000000\n",
    "\n",
    "# Select only conventional avocados\n",
    "avocado = avocado[avocado[\"type\"] == \"Conventional\"]\n",
    "\n",
    "avocado = avocado[\n",
    "    [\"date\", \"units_sold\", \"price\", \"region\", \"year\", \"month\", \"year_index\", \"peak\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "avocado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dc654-3138-4e8e-bc58-42768fc9adbd",
   "metadata": {},
   "source": [
    "## Train regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa91a90",
   "metadata": {},
   "source": [
    "We prepare the data using `OneHotEncoder` and `make_column_transformer`. We want\n",
    "to transform the region feature using the encoder while we apply scaling to the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "feat_transform = make_column_transformer(\n",
    "    (OneHotEncoder(drop=\"first\"), [\"region\"]),\n",
    "    (StandardScaler(), [\"price\", \"year_index\"]),\n",
    "    (\"passthrough\", [\"peak\"]),\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "regions = [\n",
    "    \"Great_Lakes\",\n",
    "    \"Midsouth\",\n",
    "    \"Northeast\",\n",
    "    \"Northern_New_England\",\n",
    "    \"SouthCentral\",\n",
    "    \"Southeast\",\n",
    "    \"West\",\n",
    "    \"Plains\",\n",
    "]\n",
    "df = avocado[avocado.region.isin(regions)]\n",
    "\n",
    "X = df[[\"region\", \"price\", \"year_index\", \"peak\"]]\n",
    "y = df[\"units_sold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb9ee7",
   "metadata": {},
   "source": [
    "To validate the regression model, we will randomly split the dataset into $80\\%$\n",
    "training and $20\\%$ testing data and learn the weights using `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c5ca0-17ad-46e2-95a2-8fd6f65b74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99d156-e718-4a76-b5cf-5f0f4d84b014",
   "metadata": {},
   "source": [
    "Create dictionary with various regression models that we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16294bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.base import clone\n",
    "from time import time\n",
    "regressions = {\"Linear Regression\": {\"regressor\":LinearRegression()},\n",
    "               \"MLP Regression\": {\"regressor\": MLPRegressor([8]*2, max_iter=1000)},\n",
    "               \"Decision Tree\": {\"regressor\": DecisionTreeRegressor(max_leaf_nodes=50)},\n",
    "               \"Random Forest\": {\"regressor\": RandomForestRegressor(n_estimators=10, max_leaf_nodes=100)},\n",
    "               \"Gradient Boosting\":\n",
    "               {\"regressor\" : GradientBoostingRegressor(n_estimators=100)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b7630-797b-44c2-a5a4-edac7a98745e",
   "metadata": {},
   "source": [
    "Train the regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "for regression, data in regressions.items():\n",
    "    lin_reg = make_pipeline(feat_transform,\n",
    "                            data[\"regressor\"])\n",
    "    train_start = time()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    data[\"train time\"] = time() - train_start\n",
    "    data[\"pipeline\"] = lin_reg\n",
    "\n",
    "    # Get R^2 from test data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    y_pred = lin_reg.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_pred)\n",
    "    data[\"R2 test\"] = r2_test\n",
    "    data[\"R2 train\"] = r2_train\n",
    "    print(f\"{regression:<18} R^2 value in the test set is {r2_test:.3f} training {r2_train:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c9610-3543-484b-a5b9-4cf7e7a104bf",
   "metadata": {},
   "source": [
    "Train the regressions adding polynomial features to the mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressions_poly = {}\n",
    "for regression, data in regressions.items():\n",
    "    data = {\"regressor\": clone(data[\"regressor\"])}\n",
    "    lin_reg = make_pipeline(feat_transform, PolynomialFeatures(),\n",
    "                            data[\"regressor\"])\n",
    "    train_start = time()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    data[\"train time\"] = time() - train_start\n",
    "    data[\"pipeline\"] = lin_reg\n",
    "\n",
    "    # Get R^2 from test data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    y_pred = lin_reg.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_pred)\n",
    "    data[\"R2 test\"] = r2_test\n",
    "    data[\"R2 train\"] = r2_train\n",
    "    print(f\"{regression} R^2 value in the test set is {r2_test:.3f} training {r2_train:.3f}\")\n",
    "\n",
    "    regressions_poly[f\"{regression} polynomial feats\"] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d67bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dictionary of polynomial features\n",
    "regressions |= regressions_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f48f80",
   "metadata": {},
   "source": [
    "## Prepare data of optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets and parameters\n",
    "B = 30  # total amount ot avocado supply\n",
    "\n",
    "peak_or_not = 1  # 1 if it is the peak season; 1 if isn't\n",
    "year = 2020\n",
    "\n",
    "c_waste = 0.1  # the cost ($) of wasting an avocado\n",
    "# the cost of transporting an avocado\n",
    "c_transport = pd.Series(\n",
    "    {\n",
    "        \"Great_Lakes\": 0.3,\n",
    "        \"Midsouth\": 0.1,\n",
    "        \"Northeast\": 0.4,\n",
    "        \"Northern_New_England\": 0.5,\n",
    "        \"SouthCentral\": 0.3,\n",
    "        \"Southeast\": 0.2,\n",
    "        \"West\": 0.2,\n",
    "        \"Plains\": 0.2,\n",
    "    }, name='transport_cost'\n",
    ")\n",
    "\n",
    "c_transport = c_transport.loc[regions]\n",
    "# the cost of transporting an avocado\n",
    "\n",
    "# Get the lower and upper bounds from the dataset for the price and the number of products to be stocked\n",
    "a_min = 0  # minimum avocado price in each region\n",
    "a_max = 2  # maximum avocado price in each region\n",
    "\n",
    "data = pd.concat([c_transport,\n",
    "                  df.groupby(\"region\")[\"units_sold\"].min().rename('min_delivery'),\n",
    "                  df.groupby(\"region\")[\"units_sold\"].max().rename('max_delivery')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gp.Model(\"Avocado_Price_Allocation\")\n",
    "\n",
    "x = gppd.add_vars(m, data, name=\"x\", lb='min_delivery', ub='max_delivery')\n",
    "s = gppd.add_vars(m, data, name=\"s\") # predicted amount of sales in each region for the given price).\n",
    "w = gppd.add_vars(m, data, name=\"w\") # excess wasteage in each region).\n",
    "d = gppd.add_vars(m, data, lb=-gp.GRB.INFINITY, name=\"demand\") # Add variables for the regression\n",
    "p = gppd.add_vars(m, data, name=\"price\", lb=a_min, ub=a_max)\n",
    "m.update()\n",
    "\n",
    "m.setObjective((p * s).sum() - c_waste * w.sum() - (c_transport * x).sum())\n",
    "m.ModelSense = GRB.MAXIMIZE\n",
    "\n",
    "m.addConstr(x.sum() == B)\n",
    "m.update()\n",
    "\n",
    "gppd.add_constrs(m, s, gp.GRB.LESS_EQUAL, x)\n",
    "gppd.add_constrs(m, s, gp.GRB.LESS_EQUAL, d)\n",
    "m.update()\n",
    "\n",
    "gppd.add_constrs(m, w, gp.GRB.EQUAL, x - s)\n",
    "m.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d903bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame(\n",
    "    data={\n",
    "        \"year_index\": year - 2015,\n",
    "        \"peak\": peak_or_not,\n",
    "        \"region\": regions,\n",
    "    },\n",
    "    index=regions\n",
    ")\n",
    "feats = pd.concat([feats, p], axis=1)[[\"region\", \"price\", \"year_index\", \"peak\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6452a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for regression, data in regressions.items():\n",
    "    pred_constr = add_predictor_constr(m, data[\"pipeline\"], feats, d, verbose=True)\n",
    "\n",
    "    pred_constr.print_stats()\n",
    "\n",
    "    m.Params.NonConvex = 2\n",
    "    m.write(f\"{regression}.rlp\")\n",
    "    try:\n",
    "        start = time()\n",
    "        m.optimize()\n",
    "        data[\"optimization time\"] = time() - start\n",
    "    except:\n",
    "        data[\"optimization time\"] = float('nan')\n",
    "        break\n",
    "        pass\n",
    "    pred_constr.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls -l *.rlp\n",
    "\n",
    "sizes = {' '.join(line.split()[8:])[:-4]: line.split()[4] for line in files}\n",
    "!rm *.rlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in sizes.items():\n",
    "    regressions[key][\"file size\"] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_dict(regressions, orient='index').drop([\"regressor\", \"pipeline\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f101e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"file size\"] = res[\"file size\"].astype(int)\n",
    "res[\"file size\"] /= res.loc['Linear Regression', 'file size']\n",
    "res[\"file size\"] = res[\"file size\"].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4a665",
   "metadata": {},
   "source": [
    "We have shown how to model the price and supply optimization problem with Gurobi\n",
    "Machine Learning. In the [Gurobi modeling examples\n",
    "notebook](https://github.com/Gurobi/modeling-examples/tree/master/price_optimization)\n",
    "more analysis of the solutions this model can give is done interactively. Be\n",
    "sure to take look at it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fbba6",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Copyright © 2022 Gurobi Optimization, LLC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "license": {
   "full_text": "# Copyright © 2022 Gurobi Optimization, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================="
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a00ac5",
   "metadata": {},
   "source": [
    "# Price Optimization\n",
    "\n",
    "This example is adapted from the example in Gurobi's modeling examples [How Much\n",
    "Is Too Much? Avocado Pricing and Supply Using Mathematical\n",
    "Optimization](https://github.com/Gurobi/modeling-examples/tree/master/price_optimization).\n",
    "\n",
    "We develop the same example as in the documentation but we try and compare different\n",
    "regression models to estimate demand.\n",
    "\n",
    "Note that we remove the year as a feature because it doesn't play well with SKlearn decision trees.\n",
    "\n",
    "This is mainly for testing that it works, the detailed results were not looked at and some estimators may give bogus solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e1ee1-637b-47aa-93be-c140df0610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "from gurobi_ml import add_predictor_constr\n",
    "import gurobipy_pandas as gppd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716ed7e0",
   "metadata": {},
   "source": [
    "## Load the Packages and the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666167a-ce1d-4647-be7e-1f4a56f8dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/price_optimization/\"\n",
    "avocado = pd.read_csv(\n",
    "    data_url + \"HABdata_2019_2022.csv\"\n",
    ")  # dataset downloaded directly from HAB\n",
    "avocado_old = pd.read_csv(\n",
    "    data_url + \"kaggledata_till2018.csv\"\n",
    ")  # dataset downloaded from Kaggle\n",
    "avocado = pd.concat([avocado, avocado_old])\n",
    "\n",
    "# Add the index for each year from 2015 through 2022\n",
    "avocado[\"date\"] = pd.to_datetime(avocado[\"date\"])\n",
    "avocado[\"year\"] = pd.DatetimeIndex(avocado[\"date\"]).year\n",
    "avocado[\"year_index\"] = avocado[\"year\"] - 2015\n",
    "avocado = avocado.sort_values(by=\"date\")\n",
    "\n",
    "# Define the peak season\n",
    "avocado[\"month\"] = pd.DatetimeIndex(avocado[\"date\"]).month\n",
    "peak_months = range(2, 8)  # <--------- Set the months for the \"peak season\"\n",
    "\n",
    "\n",
    "def peak_season(row):\n",
    "    return 1 if int(row[\"month\"]) in peak_months else 0\n",
    "\n",
    "\n",
    "avocado[\"peak\"] = avocado.apply(lambda row: peak_season(row), axis=1)\n",
    "\n",
    "# Scale the number of avocados to millions\n",
    "avocado[\"units_sold\"] = avocado[\"units_sold\"] / 1000000\n",
    "\n",
    "# Select only conventional avocados\n",
    "avocado = avocado[avocado[\"type\"] == \"Conventional\"]\n",
    "\n",
    "avocado = avocado[\n",
    "    [\"date\", \"units_sold\", \"price\", \"region\", \"year\", \"month\", \"year_index\", \"peak\"]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "avocado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dc654-3138-4e8e-bc58-42768fc9adbd",
   "metadata": {},
   "source": [
    "## Train regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa91a90",
   "metadata": {},
   "source": [
    "We prepare the data using `OneHotEncoder` and `make_column_transformer`. We want\n",
    "to transform the region feature using the encoder while we apply scaling to the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "feat_transform = make_column_transformer(\n",
    "    (OneHotEncoder(drop=\"first\"), [\"region\"]),\n",
    "    (StandardScaler(), [\"price\"]),\n",
    "    (\"passthrough\", [\"peak\"]),\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "regions = [\n",
    "    \"Great_Lakes\",\n",
    "    \"Midsouth\",\n",
    "    \"Northeast\",\n",
    "    \"Northern_New_England\",\n",
    "    \"SouthCentral\",\n",
    "    \"Southeast\",\n",
    "    \"West\",\n",
    "    \"Plains\",\n",
    "]\n",
    "df = avocado[avocado.region.isin(regions)]\n",
    "\n",
    "X = df[[\"region\", \"price\", \"peak\"]]\n",
    "y = df[\"units_sold\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb9ee7",
   "metadata": {},
   "source": [
    "To validate the regression model, we will randomly split the dataset into $80\\%$\n",
    "training and $20\\%$ testing data and learn the weights using `Scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c5ca0-17ad-46e2-95a2-8fd6f65b74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e99d156-e718-4a76-b5cf-5f0f4d84b014",
   "metadata": {},
   "source": [
    "Create dictionary with various regression models that we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16294bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from time import time\n",
    "args={\"random_state\":1}\n",
    "regressions = {\"Linear Regression\": {\"regressor\":LinearRegression() },\n",
    "               \"MLP Regression\": {\"regressor\": MLPRegressor([8]*2, max_iter=1000, **args)},\n",
    "               \"Decision Tree\": {\"regressor\": DecisionTreeRegressor(max_leaf_nodes=50, **args)},\n",
    "               \"Random Forest\": {\"regressor\": RandomForestRegressor(n_estimators=10, max_leaf_nodes=100, **args)},\n",
    "               \"Gradient Boosting\":\n",
    "               {\"regressor\" : GradientBoostingRegressor(n_estimators=20, **args)},\n",
    "              \"XGB Regressor\": {\"regressor\": XGBRegressor(n_estimators=20, **args)}}\n",
    "\n",
    "# Add polynomial features for linear regression and MLP\n",
    "regressions_poly = {}\n",
    "for regression in [\"Linear Regression\", \"MLP Regression\"]:\n",
    "    data = {\"regressor\": (PolynomialFeatures(),clone(regressions[regression][\"regressor\"]))}\n",
    "    regressions_poly[f\"{regression} polynomial feats\"] = data\n",
    "# Merge dictionary of polynomial features\n",
    "regressions |= regressions_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b7630-797b-44c2-a5a4-edac7a98745e",
   "metadata": {},
   "source": [
    "Train the regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cb443",
   "metadata": {},
   "outputs": [],
   "source": [
    "for regression, data in regressions.items():\n",
    "    regressor = data[\"regressor\"]\n",
    "    if isinstance(regressor, tuple):\n",
    "        lin_reg = make_pipeline(feat_transform,\n",
    "                                *regressor)\n",
    "    else:\n",
    "        lin_reg = make_pipeline(feat_transform,\n",
    "                                regressor)\n",
    "    train_start = time()\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    data[(\"Learning\",\"time\")] = time() - train_start\n",
    "    data[\"pipeline\"] = lin_reg\n",
    "\n",
    "    # Get R^2 from test data\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    r2_test = r2_score(y_test, y_pred)\n",
    "    y_pred = lin_reg.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_pred)\n",
    "    data[(\"Learning\", \"R2 test\")] = r2_test\n",
    "    data[(\"Learning\", \"R2 train\")] = r2_train\n",
    "    print(f\"{regression:<18} R^2 value in the test set is {r2_test:.3f} training {r2_train:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f48f80",
   "metadata": {},
   "source": [
    "## Prepare data of optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets and parameters\n",
    "B = 30  # total amount ot avocado supply\n",
    "\n",
    "peak_or_not = 1  # 1 if it is the peak season; 1 if isn't\n",
    "\n",
    "c_waste = 0.1  # the cost ($) of wasting an avocado\n",
    "# the cost of transporting an avocado\n",
    "c_transport = pd.Series(\n",
    "    {\n",
    "        \"Great_Lakes\": 0.3,\n",
    "        \"Midsouth\": 0.1,\n",
    "        \"Northeast\": 0.4,\n",
    "        \"Northern_New_England\": 0.5,\n",
    "        \"SouthCentral\": 0.3,\n",
    "        \"Southeast\": 0.2,\n",
    "        \"West\": 0.2,\n",
    "        \"Plains\": 0.2,\n",
    "    }, name='transport_cost'\n",
    ")\n",
    "\n",
    "c_transport = c_transport.loc[regions]\n",
    "# the cost of transporting an avocado\n",
    "\n",
    "# Get the lower and upper bounds from the dataset for the price and the number of products to be stocked\n",
    "a_min = 0  # minimum avocado price in each region\n",
    "a_max = 2  # maximum avocado price in each region\n",
    "\n",
    "data = pd.concat([c_transport,\n",
    "                  df.groupby(\"region\")[\"units_sold\"].min().rename('min_delivery'),\n",
    "                  df.groupby(\"region\")[\"units_sold\"].max().rename('max_delivery')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bcc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e65ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gp.Model(\"Avocado_Price_Allocation\")\n",
    "\n",
    "p = gppd.add_vars(m, data, name=\"price\", lb=a_min, ub=a_max)\n",
    "d = gppd.add_vars(m, data, name=\"demand\") # Add variables for the regression\n",
    "w = m.addVar(name=\"w\") # excess wasteage\n",
    "m.update()\n",
    "\n",
    "m.setObjective((p * d).sum() - c_waste * w - (c_transport * d).sum())\n",
    "m.ModelSense = GRB.MAXIMIZE\n",
    "\n",
    "m.addConstr(d.sum() + w == B)\n",
    "m.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d903bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.DataFrame(\n",
    "    data={\n",
    "        \"peak\": peak_or_not,\n",
    "        \"region\": regions,\n",
    "        \"price\": p\n",
    "    },\n",
    "    index=regions\n",
    ")\n",
    "feats = feats[[\"region\", \"price\", \"peak\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6452a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for regression, data in regressions.items():\n",
    "    pred_constr = add_predictor_constr(m, data[\"pipeline\"], feats, d, epsilon=1e-5)\n",
    "\n",
    "    pred_constr.print_stats()\n",
    "\n",
    "    data[(\"Optimization\", \"#constrs\")] = m.NumConstrs + m.NumQConstrs + m.NumGenConstrs\n",
    "    data[(\"Optimization\", \"#vars\")] = m.NumVars\n",
    "    m.Params.NonConvex = 2\n",
    "    m.Params.OutputFlag = 0\n",
    "    try:\n",
    "        start = time()\n",
    "        m.optimize()\n",
    "        data[(\"Optimization\", \"time\")] = time() - start\n",
    "        if m.Status in (gp.GRB.INFEASIBLE, gp.GRB.INF_OR_UNBD):\n",
    "            data[(\"Optimization\", \"value\")] = float('nan')\n",
    "            data[(\"Optimization\", \"viol\")] = float('nan')\n",
    "            data[(\"Optimization\", \"error\")] = float('nan')\n",
    "        else:\n",
    "            data[(\"Optimization\", \"value\")] = m.ObjVal\n",
    "            data[(\"Optimization\", \"viol\")] = m.MaxVio\n",
    "            data[(\"Optimization\", \"error\")] = pred_constr.get_error().max()\n",
    "    except:\n",
    "        data[(\"Optimization\", \"value\")] = float('nan')\n",
    "        data[(\"Optimization\", \"viol\")] = float('nan')\n",
    "        data[(\"Optimization\", \"error\")] = float('nan')\n",
    "        break\n",
    "        pass\n",
    "    pred_constr.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a7cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_dict(regressions, orient='index').drop([\"regressor\", \"pipeline\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns = pd.MultiIndex.from_tuples(res.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fbba6",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Copyright © 2022 Gurobi Optimization, LLC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "license": {
   "full_text": "# Copyright © 2023 Gurobi Optimization, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================="
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30674e2a",
   "metadata": {},
   "source": [
    "# Surrogate Models\n",
    "\n",
    "Some industrial applications require modeling complex processes that can result\n",
    "either in highly non-linear functions or functions defined by a simulation\n",
    "process. In those contexts, optimization solvers often struggle. The reason may\n",
    "be that relaxations of the nonlinear functions are not good enough to make the\n",
    "solver prove an acceptable bound in a reasonable amount of time. Another issue\n",
    "may be that the solver is not able to represent the functions.\n",
    "\n",
    "An approach that has been proposed in the literature is to approximate the\n",
    "problematic nonlinear functions via neural networks with ReLU activation and use\n",
    "MIP technology to solve the constructed approximation (see e.g. <cite\n",
    "data-cite=\"Henao_Maravelias_2011\">[Heneao Maravelias\n",
    "2011](https://doi.org/https://doi.org/10.1002/aic.12341)</cite>, <cite\n",
    "data-cite=\"Schweidtmann_2022\"> [Schweitdmann et.al.\n",
    "2022](https://arxiv.org/abs/2207.12722)</cite>). This use of neural network can\n",
    "be motivated by their ability to provide a universal approximation (see e.g.\n",
    "<cite data-cite=\"Lu_Pu_2017\">[Lu et.al.\n",
    "2017](https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf)</cite>).\n",
    "This use of ML models to replace complex processes is often referred to as\n",
    "*surrogate models*.\n",
    "\n",
    "In the following example, we approximate a nonlinear function via `Scikit-learn`\n",
    "`MLPRegressor` and then solve an optimization problem that uses the\n",
    "approximation of the nonlinear function with Gurobi.\n",
    "\n",
    "The purpose of this example is solely illustrative and doesn't relate to any\n",
    "particular application.\n",
    "\n",
    "The function we approximate is the [2D peaks\n",
    "function](https://www.mathworks.com/help/matlab/ref/peaks.html#mw_46aeee28-390e-4373-aa47-e4a52447fc85).\n",
    "\n",
    "The function is given as\n",
    "\n",
    "$$ \\begin{aligned} f(x) = & 3 \\cdot (1-x_1)^2 \\cdot \\exp(-x_1^2 - (x_2+1)^2) -\n",
    "\\\\\n",
    "         & 10 \\cdot (\\frac{x_1}{5} - x_1^3 - x_2^5) \\cdot \\exp(-x_1^2 - x_2^2) - \\\\\n",
    "         & \\frac{1}{3} \\cdot \\exp(-(x_1+1)^2 - x_2^2).\n",
    "\\end{aligned} $$\n",
    "\n",
    "In this example, we want to find the minimum of $f$ over the interval $[-2, 2]$:\n",
    "\n",
    "$$ y = \\min \\{f(x) : x \\in [-2,2]^2\\}. $$\n",
    "\n",
    "The [global minimum of this problem can be found\n",
    "numerically](<https://www.math.uwaterloo.ca/~hwolkowi/henry/reports/talks.d/t09talks.d/09waterloomatlab.d/optimTipsWebinar/html/optimTipsTricksWalkthrough.html#18>)\n",
    "to have value $-6.55113$ at the point $(0.2283, -1.6256)$.\n",
    "\n",
    "Here to find this minimum of $f$, we approximate $f(x)$ through a neural network\n",
    "function $g(x)$ to obtain a MIP and solve\n",
    "\n",
    "$$ \\hat y = \\min \\{g(x) : x \\in [-2,2]^2\\} \\approx y. $$\n",
    "\n",
    "First import the necessary packages. Before applying the neural network, we do a\n",
    "preprocessing to extract polynomial features of degree 2. Hopefully this will\n",
    "help us to approximate the smooth function. Besides, `gurobipy`, `numpy` and the\n",
    "appropriate `sklearn` objects, we also use `matplotlib` to plot the function,\n",
    "and it's approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "import numpy as np\n",
    "from gurobipy import GRB\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from gurobi_ml import add_predictor_constr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee6aad",
   "metadata": {},
   "source": [
    "## Define the nonlinear function of interest\n",
    "\n",
    "We define the 2D peak function as a python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak2d(x1, x2):\n",
    "    return (\n",
    "        3 * (1 - x1) ** 2.0 * np.exp(-(x1**2) - (x2 + 1) ** 2)\n",
    "        - 10 * (x1 / 5 - x1**3 - x2**5) * np.exp(-(x1**2) - x2**2)\n",
    "        - 1 / 3 * np.exp(-((x1 + 1) ** 2) - x2**2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bf916",
   "metadata": {},
   "source": [
    "To train the neural network, we make a uniform sample of the domain of the\n",
    "function in the region of interest using `numpy`'s `arrange` function.\n",
    "\n",
    "We then plot the function with `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fdd5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = np.meshgrid(np.arange(-2, 2, 0.01), np.arange(-2, 2, 0.01))\n",
    "y = peak2d(x1, x2)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x1, x2, y, cmap=cm.coolwarm, linewidth=0.01, antialiased=False)\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2673184d",
   "metadata": {},
   "source": [
    "## Approximate the function\n",
    "\n",
    "To fit a model, we need to reshape our data. We concatenate the values of `x1`\n",
    "and `x2` in an array `X` and make `y` one dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902d8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([x1.ravel().reshape(-1, 1), x2.ravel().reshape(-1, 1)], axis=1)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b6491",
   "metadata": {},
   "source": [
    "To approximate the function, we use a `Pipeline` with polynomial features and a\n",
    "neural-network regressor. We do a relatively small neural-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a260ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our regression\n",
    "layers = [30] * 2\n",
    "regression = MLPRegressor(hidden_layer_sizes=layers, activation=\"relu\")\n",
    "pipe = make_pipeline(PolynomialFeatures(), regression)\n",
    "pipe.fit(X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f569cff",
   "metadata": {},
   "source": [
    "To test the accuracy of the approximation, we take a random sample of points,\n",
    "and we print the $R^2$ value and the maximal error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.random.random((100, 2)) * 4 - 2\n",
    "\n",
    "r2_score = metrics.r2_score(peak2d(X_test[:, 0], X_test[:, 1]), pipe.predict(X_test))\n",
    "max_error = metrics.max_error(peak2d(X_test[:, 0], X_test[:, 1]), pipe.predict(X_test))\n",
    "print(\"R2 error {}, maximal error {}\".format(r2_score, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7cf08",
   "metadata": {},
   "source": [
    "While the $R^2$ value is good, the maximal error is quite high. For the purpose\n",
    "of this example we still deem it acceptable. We plot the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(\n",
    "    x1,\n",
    "    x2,\n",
    "    pipe.predict(X).reshape(x1.shape),\n",
    "    cmap=cm.coolwarm,\n",
    "    linewidth=0.01,\n",
    "    antialiased=False,\n",
    ")\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a7089",
   "metadata": {},
   "source": [
    "Visually, the approximation looks close enough to the original function.\n",
    "\n",
    "## Build and Solve the Optimization Model\n",
    "\n",
    "We now turn to the optimization model. For this model we want to find the\n",
    "minimal value of `y_approx` which is the approximation given by our pipeline on\n",
    "the interval.\n",
    "\n",
    "Note that in this simple example, we don't use matrix variables but regular\n",
    "Gurobi variables instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gp.Model()\n",
    "\n",
    "x = m.addVars(2, lb=-2, ub=2, name=\"x\")\n",
    "y_approx = m.addVar(lb=-GRB.INFINITY, name=\"y\")\n",
    "\n",
    "m.setObjective(y_approx, gp.GRB.MINIMIZE)\n",
    "\n",
    "# add \"surrogate constraint\"\n",
    "pred_constr = add_predictor_constr(m, pipe, x, y_approx)\n",
    "\n",
    "pred_constr.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadb1d6",
   "metadata": {},
   "source": [
    "Now call `optimize`. Since we use polynomial features the resulting model is a\n",
    "non-convex quadratic problem. In Gurobi, we need to set the parameter\n",
    "`NonConvex` to 2 to be able to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef66881",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.Params.TimeLimit = 20\n",
    "m.Params.MIPGap = 0.1\n",
    "m.Params.NonConvex = 2\n",
    "\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bebac4",
   "metadata": {},
   "source": [
    "After solving the model, we check the error in the estimate of the Gurobi\n",
    "solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ec658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error in approximating the regression {:.6}\".format(np.max(np.abs(pred_constr.get_error()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd212307",
   "metadata": {},
   "source": [
    "Finally, we look at the solution and the objective value found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fba5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"solution point of the approximated problem ({x[0].X:.4}, {x[1].X:.4}), \"\n",
    "    + f\"objective value {m.ObjVal}.\"\n",
    ")\n",
    "print(\n",
    "    f\"Function value at the solution point {peak2d(x[0].X, x[1].X)} error {abs(peak2d(x[0].X, x[1].X) - m.ObjVal)}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c7a4e",
   "metadata": {},
   "source": [
    "The difference between the function and the approximation at the computed\n",
    "solution point is noticeable, but the point we found is reasonably close to the\n",
    "actual global minimum. Depending on the use case this might be deemed\n",
    "acceptable. Of course, training a larger network should result in a better\n",
    "approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e1920",
   "metadata": {},
   "source": [
    "Copyright © 2020 Gurobi Optimization, LLC"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb///ipynb,myst///md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "license": {
   "full_text": "# Copyright © 2022 Gurobi Optimization, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================="
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
